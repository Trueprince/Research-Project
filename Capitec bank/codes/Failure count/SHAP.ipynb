{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0413729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# =========================\n",
    "# 0Ô∏è‚É£ SentenceTransformer for embeddings\n",
    "# =========================\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# =========================\n",
    "# 1Ô∏è‚É£ Pick the instance to explain\n",
    "# =========================\n",
    "instance_idx = 1\n",
    "instance_row = focus_df.iloc[instance_idx]\n",
    "text_instance = instance_row['Content']\n",
    "instance_total = instance_row['Total_Count']\n",
    "print(\"üìÖ Date of instance:\", instance_row['Day'])\n",
    "print(\"üìÑ Text:\", text_instance)\n",
    "\n",
    "# =========================\n",
    "# 2Ô∏è‚É£ Convert text to combined embeddings\n",
    "# =========================\n",
    "def text_to_combined_vector(text_list):\n",
    "    embed_vecs = sentence_model.encode(text_list)\n",
    "    counts = vectorizer.transform(text_list)\n",
    "    lda_vecs = lda_final.transform(counts)\n",
    "    return np.hstack([embed_vecs, lda_vecs])\n",
    "\n",
    "# =========================\n",
    "# 3Ô∏è‚É£ Fit scaler on embeddings (inputs)\n",
    "# =========================\n",
    "train_embeddings = text_to_combined_vector(focus_df['Content'].tolist())\n",
    "scaler_embed = StandardScaler().fit(train_embeddings)\n",
    "\n",
    "# =========================\n",
    "# 4Ô∏è‚É£ SHAP prediction wrapper (scale inputs, inverse scale output)\n",
    "# =========================\n",
    "def model_predict_shap(text_list, return_percent=False):\n",
    "    # --- Convert text to embeddings\n",
    "    X_vec = text_to_combined_vector(text_list)\n",
    "    # --- Scale embeddings (as model was trained)\n",
    "    X_scaled = scaler_embed.transform(X_vec)\n",
    "    \n",
    "    # --- Pad or trim to match model input features\n",
    "    target_dim = 192\n",
    "    curr_dim = X_scaled.shape[1]\n",
    "    if curr_dim < target_dim:\n",
    "        pad = np.zeros((X_scaled.shape[0], target_dim - curr_dim))\n",
    "        X_scaled = np.hstack([X_scaled, pad])\n",
    "    else:\n",
    "        X_scaled = X_scaled[:, :target_dim]\n",
    "\n",
    "    # --- Repeat for sequence input (GRU expects seq_len=3)\n",
    "    seq_len = 3\n",
    "    X_seq = np.array([np.tile(X_scaled[i], (seq_len, 1)) for i in range(len(X_scaled))], dtype=np.float32)\n",
    "    \n",
    "    # --- Predict scaled output\n",
    "    preds_scaled = model.predict(X_seq, verbose=0).flatten()\n",
    "    \n",
    "    # --- Inverse scale to original failure counts\n",
    "    preds_rescaled = scaler_y.inverse_transform(preds_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    if return_percent:\n",
    "        preds_percent = preds_rescaled / np.array([instance_total]*len(preds_rescaled)) * 100\n",
    "        return preds_percent\n",
    "    return preds_rescaled\n",
    "\n",
    "# =========================\n",
    "# 5Ô∏è‚É£ SHAP explainer\n",
    "# =========================\n",
    "masker = shap.maskers.Text(\" \")\n",
    "explainer = shap.Explainer(model_predict_shap, masker)\n",
    "\n",
    "# =========================\n",
    "# 6Ô∏è‚É£ Explain selected instance\n",
    "# =========================\n",
    "shap_values = explainer([text_instance])\n",
    "\n",
    "# =========================\n",
    "# 7Ô∏è‚É£ Visualize\n",
    "# =========================\n",
    "shap.plots.text(shap_values[0])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
